I"^<h2 id="demo-video">Demo Video</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/8eEAvcy708s" frameborder="0" allowfullscreen=""></iframe>

<h2 id="re-writing-avfoundation-code-can-be-a-hassle">Re-Writing AVFoundation Code Can Be A Hassle</h2>

<p>It’s not that <code class="highlighter-rouge">AVFoundation</code> is bad. <code class="highlighter-rouge">AVFoundation</code>, in fact, is very very good. Great, even.</p>

<p>However, it’s not ideal to have to write the same chunk of code over and over again. In my experience, I’ve had to do this a lot when it comes to cameras. Having to re-write logic around filtering company names is one thing - this is a bit more…involved.</p>

<p>Then came <code class="highlighter-rouge">CoreML</code> in iOS 11. This rules! It’s put the power of machine learning in the hands of a mobile developer, and it’s quite easy to get started. Audrey Tam from <a href="https://www.raywenderlich.com/u/audrey">raywenderlich.com</a> wrote a really great tutorial on getting started with <code class="highlighter-rouge">Vision</code> and <code class="highlighter-rouge">CoreML</code>, and you can read it <a href="https://www.raywenderlich.com/164213/coreml-and-vision-machine-learning-in-ios-11-tutorial">here</a>.</p>

<p>As neat as this tutorial is, I still was struck by how much code you had to write in order to get to a place where you can recognize a single still image. This is where my framework, <a href="https://github.com/dokun1/Lumina">Lumina</a> comes in.</p>

<h2 id="what-does-lumina-do">What Does Lumina Do?</h2>

<p>Lumina can be a drop-in camera framework for you in apps that need iOS 10 or above. Aside from <code class="highlighter-rouge">CoreML</code>, its features are:</p>

<ul>
  <li>configurable pinch to zoom</li>
  <li>video &amp; still image capture</li>
  <li>image streaming via delegate (for post-processing)</li>
  <li>live photo capture</li>
  <li>configurable frame rate</li>
  <li>QR code/barcode/face detection</li>
  <li>capture &amp; stream <code class="highlighter-rouge">AVDepthData</code> (for iOS 11)</li>
  <li>configurable text prompt view</li>
  <li>configurable logging</li>
</ul>

<p>I still have many features I’d like to add over time, such as configurable exposure, white balance, and others! If you’d like to contribute, feel free to dive in.</p>

<h2 id="ok-butcoreml">Ok But…CoreML?</h2>

<p>So here’s what my favorite part of working with Lumina has been. Let’s say you made an instance of Lumina like so:</p>

<div class="language-swift highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">let</span> <span class="nv">camera</span> <span class="o">=</span> <span class="kt">LuminaViewController</span><span class="p">()</span>
<span class="n">camera</span><span class="o">.</span><span class="n">delegate</span> <span class="o">=</span> <span class="k">self</span>
<span class="nf">present</span><span class="p">(</span><span class="n">camera</span><span class="p">,</span> <span class="nv">animated</span><span class="p">:</span> <span class="kc">true</span><span class="p">)</span>
</code></pre></div></div>

<p>Now let’s say you have a model called <code class="highlighter-rouge">MobileNet.mlmodel</code> that you want to stream the video frames from Lumina through. After you import that model to your Xcode project, here’s all you have to do to get it streaming frames:</p>

<div class="language-swift highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">camera</span><span class="o">.</span><span class="n">streamingModelTypes</span> <span class="o">=</span> <span class="p">[</span><span class="kt">MobileNet</span><span class="p">()]</span>
</code></pre></div></div>

<p>That’s it! You’ve done the heavy lifting! Now, results will be streamed back through this function in your delegate:</p>

<div class="language-swift highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">func</span> <span class="nf">streamed</span><span class="p">(</span><span class="nv">videoFrame</span><span class="p">:</span> <span class="kt">UIImage</span><span class="p">,</span> <span class="n">with</span> <span class="nv">predictions</span><span class="p">:</span> <span class="p">[([</span><span class="kt">LuminaPrediction</span><span class="p">]?,</span> <span class="kt">Any</span><span class="o">.</span><span class="k">Type</span><span class="p">)]?,</span> <span class="n">from</span> <span class="nv">controller</span><span class="p">:</span> <span class="kt">LuminaViewController</span><span class="p">)</span> <span class="p">{</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">predictions</span><span class="o">!</span><span class="p">)</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Since this framework is written to work with iOS 11.0 and above, you’ll have to iterate through the tuple <code class="highlighter-rouge">predictions</code> to see which model your results are associated with, but this means you can load as many models as you want into Lumina, and each video frame will be processed through the collection of models.</p>

<h2 id="conclusion">Conclusion</h2>

<p>I released a video that walks through how to set up and use Lumina. You can watch it again here:</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/8eEAvcy708s" frameborder="0" allowfullscreen=""></iframe>

<p>Please feel free to check the repository <a href="https://github.com/dokun1/Lumina">here</a> if you want to check the progress of the framework, or if you want to help out - I’d love to see other contributors hop on!</p>
:ET